{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "J2OISlMFyBuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)"
      ],
      "metadata": {
        "id": "TsxA-ZR2Qx-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = random.Random()\n",
        "lim_unigram = 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90"
      ],
      "metadata": {
        "id": "MMRnz-X7yZyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_train_instances, file_train_bodies, file_test_instances, file_test_bodies):\n",
        "    train_instances = pd.read_csv(file_train_instances)\n",
        "    train_bodies = pd.read_csv(file_train_bodies)\n",
        "    test_instances = pd.read_csv(file_test_instances)\n",
        "    test_bodies = pd.read_csv(file_test_bodies)\n",
        "    return train_instances, train_bodies, test_instances, test_bodies"
      ],
      "metadata": {
        "id": "xw196Hv0ybXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_text(data, bodies):\n",
        "    # Convert 'Body ID' columns to the same type (string)\n",
        "    data['Body ID'] = data['Body ID'].astype(str)\n",
        "    bodies['Body ID'] = bodies['Body ID'].astype(str)\n",
        "\n",
        "    # Merge headline and body ID with the actual body text\n",
        "    data = data.merge(bodies, how='left', on='Body ID')\n",
        "    return data"
      ],
      "metadata": {
        "id": "O3boa6lhye6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_train(train_instances, train_bodies, lim_unigram=lim_unigram):\n",
        "    # Concatenate text for vectorization\n",
        "    train_instances = concatenate_text(train_instances, train_bodies)\n",
        "\n",
        "    # Separate headline and body text\n",
        "    headlines = train_instances['Headline'].astype(str)\n",
        "    bodies = train_instances['articleBody'].astype(str)\n",
        "\n",
        "    # Vectorization\n",
        "    bow_vectorizer_headline = CountVectorizer(max_features=lim_unigram)\n",
        "    bow_vectorizer_body = CountVectorizer(max_features=lim_unigram)\n",
        "    tfidf_vectorizer_headline = TfidfVectorizer(max_features=lim_unigram)\n",
        "    tfidf_vectorizer_body = TfidfVectorizer(max_features=lim_unigram)\n",
        "\n",
        "    # Fit and transform\n",
        "    bow_headline = bow_vectorizer_headline.fit_transform(headlines)\n",
        "    bow_body = bow_vectorizer_body.fit_transform(bodies)\n",
        "    tfidf_headline = tfidf_vectorizer_headline.fit_transform(headlines)\n",
        "    tfidf_body = tfidf_vectorizer_body.fit_transform(bodies)\n",
        "\n",
        "    # Determine the feature size\n",
        "    feature_size_headline = bow_headline.shape[1] + tfidf_headline.shape[1]\n",
        "    feature_size_body = bow_body.shape[1] + tfidf_body.shape[1]\n",
        "\n",
        "    # Prepare feature set\n",
        "    train_set_headline = np.concatenate([bow_headline.toarray(), tfidf_headline.toarray()], axis=1)\n",
        "    train_set_body = np.concatenate([bow_body.toarray(), tfidf_body.toarray()], axis=1)\n",
        "    train_stances = train_instances['Stance'].values\n",
        "\n",
        "    print(\"Train set headline shape:\", train_set_headline.shape)\n",
        "    print(\"Train set body shape:\", train_set_body.shape)\n",
        "\n",
        "    return train_set_headline, train_set_body, train_stances, bow_vectorizer_headline, bow_vectorizer_body, tfidf_vectorizer_headline, tfidf_vectorizer_body, feature_size_headline, feature_size_body"
      ],
      "metadata": {
        "id": "t6BFbrF7yfhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_test(test_instances, test_bodies, bow_vectorizer_headline, bow_vectorizer_body, tfidf_vectorizer_headline, tfidf_vectorizer_body):\n",
        "    test_instances = concatenate_text(test_instances, test_bodies)\n",
        "\n",
        "    # Separate headline and body text\n",
        "    headlines = test_instances['Headline'].astype(str)\n",
        "    bodies = test_instances['articleBody'].astype(str)\n",
        "\n",
        "    bow_headline = bow_vectorizer_headline.transform(headlines)\n",
        "    bow_body = bow_vectorizer_body.transform(bodies)\n",
        "    tfidf_headline = tfidf_vectorizer_headline.transform(headlines)\n",
        "    tfidf_body = tfidf_vectorizer_body.transform(bodies)\n",
        "\n",
        "    test_set_headline = np.concatenate([bow_headline.toarray(), tfidf_headline.toarray()], axis=1)\n",
        "    test_set_body = np.concatenate([bow_body.toarray(), tfidf_body.toarray()], axis=1)\n",
        "\n",
        "    print(\"Test set headline shape:\", test_set_headline.shape)\n",
        "    print(\"Test set body shape:\", test_set_body.shape)\n",
        "\n",
        "    return test_set_headline, test_set_body"
      ],
      "metadata": {
        "id": "diinNCj-yg2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(inputs, attention_size):\n",
        "    hidden_size = inputs.shape[-1]  # D value - hidden size of the input\n",
        "    w_omega = tf.Variable(tf.random.normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
        "\n",
        "    #Weight matrix for transforming the input vectors.\n",
        "    #bias vector added to the transformed input.\n",
        "    #Context vector used to compute the importance scores for the input vectors.\n",
        "\n",
        "\n",
        "    v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # [B, T, attention_size] * [attention_size] -> [B, T]\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')  # (B, T) shape #normalised attention weights\n",
        "\n",
        "    tf.print(\"Alphas shape:\", tf.shape(alphas))\n",
        "    tf.print(\"Alphas values:\", alphas)\n",
        "\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), axis=1) #weighted sum of input vectors\n",
        "    return output"
      ],
      "metadata": {
        "id": "FUJDxaiLNhCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(feature_size_headline, feature_size_body, hidden_size, attention_size, target_size, learning_rate):\n",
        "    # Define placeholders\n",
        "    headline_pl = tf.compat.v1.placeholder(tf.float32, [None, feature_size_headline], 'headline')\n",
        "    body_pl = tf.compat.v1.placeholder(tf.float32, [None, feature_size_body], 'body')\n",
        "    stances_pl = tf.compat.v1.placeholder(tf.int64, [None], 'stances')\n",
        "    keep_prob_pl = tf.compat.v1.placeholder(tf.float32)\n",
        "\n",
        "    # Apply attention mechanism to headline and body separately\n",
        "    headline_expanded = tf.expand_dims(headline_pl, -1)\n",
        "    body_expanded = tf.expand_dims(body_pl, -1)\n",
        "    attention_headline = attention(headline_expanded, attention_size)\n",
        "    attention_body = attention(body_expanded, attention_size)\n",
        "\n",
        "    # Concatenate the attended representations\n",
        "    combined_representation = tf.concat([attention_headline, attention_body], axis=1)\n",
        "\n",
        "    # Define the rest of the model\n",
        "    hidden_layer = tf.nn.dropout(tf.nn.relu(tf.compat.v1.layers.dense(combined_representation, hidden_size)), rate=1-keep_prob_pl)\n",
        "    logits_flat = tf.nn.dropout(tf.compat.v1.layers.dense(hidden_layer, target_size), rate=1-keep_prob_pl)\n",
        "    loss = tf.reduce_sum(tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=stances_pl))\n",
        "\n",
        "    # Optimizer\n",
        "    train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "    return headline_pl, body_pl, stances_pl, keep_prob_pl, logits_flat, loss, train_op"
      ],
      "metadata": {
        "id": "kwzHhNdOyknP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(sess, checkpoint_dir):\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    if latest_checkpoint:\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        try:\n",
        "            saver.restore(sess, latest_checkpoint)\n",
        "            print(\"Model restored from\", latest_checkpoint)\n",
        "        except tf.errors.NotFoundError as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            # Initialize any variables that were not restored\n",
        "            uninitialized_vars = sess.run(tf.compat.v1.report_uninitialized_variables())\n",
        "            if uninitialized_vars:\n",
        "                sess.run(tf.compat.v1.variables_initializer(uninitialized_vars))\n",
        "                print(\"Initialized missing variables.\")\n",
        "    else:\n",
        "        raise ValueError(f\"No valid checkpoint found in the directory: {checkpoint_dir}\")\n"
      ],
      "metadata": {
        "id": "nHbIZCVRyn_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    mode = 'load'  # or any other mode you have\n",
        "    model_dir = '/content/model'\n",
        "    hidden_size = 100\n",
        "    attention_size = 50\n",
        "    target_size = 4\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    # Load and process data\n",
        "    file_train_instances = \"train_stances.csv\"\n",
        "    file_train_bodies = \"train_bodies.csv\"\n",
        "    file_test_instances = \"competition_test_stances_unlabeled.csv\"\n",
        "    file_test_bodies = \"test_bodies.csv\"\n",
        "\n",
        "    raw_train, raw_train_bodies, raw_test, raw_test_bodies = load_data(file_train_instances, file_train_bodies, file_test_instances, file_test_bodies)\n",
        "    train_set_headline, train_set_body, train_stances, bow_vectorizer_headline, bow_vectorizer_body, tfidf_vectorizer_headline, tfidf_vectorizer_body, feature_size_headline, feature_size_body = pipeline_train(raw_train, raw_train_bodies, lim_unigram=lim_unigram)\n",
        "    test_set_headline, test_set_body = pipeline_test(raw_test, raw_test_bodies, bow_vectorizer_headline, bow_vectorizer_body, tfidf_vectorizer_headline, tfidf_vectorizer_body)\n",
        "\n",
        "    if mode == 'load':\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            headline_pl, body_pl, stances_pl, keep_prob_pl, logits_flat, loss, train_op = build_model(feature_size_headline, feature_size_body, hidden_size, attention_size, target_size, learning_rate)\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())  # Initialize all variables first\n",
        "            load_model(sess, model_dir)\n",
        "\n",
        "            # Predict\n",
        "            test_feed_dict = {headline_pl: test_set_headline, body_pl: test_set_body, keep_prob_pl: 1.0}\n",
        "            test_pred = sess.run(logits_flat, feed_dict=test_feed_dict)\n",
        "            print(\"Predictions:\", test_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IFjcxeaypxo",
        "outputId": "ab7eaff7-1843-4b3d-f3e7-02ddab764bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set headline shape: (11998, 6568)\n",
            "Train set body shape: (11998, 10000)\n",
            "Test set headline shape: (558, 6568)\n",
            "Test set body shape: (558, 10000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-fd75bf4be2cc>:18: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  hidden_layer = tf.nn.dropout(tf.nn.relu(tf.compat.v1.layers.dense(combined_representation, hidden_size)), rate=1-keep_prob_pl)\n",
            "<ipython-input-43-fd75bf4be2cc>:19: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits_flat = tf.nn.dropout(tf.compat.v1.layers.dense(hidden_layer, target_size), rate=1-keep_prob_pl)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading checkpoint: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
            "\n",
            "Graph execution error:\n",
            "\n",
            "Detected at node 'save_3/RestoreV2' defined at (most recent call last):\n",
            "    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    File \"<ipython-input-45-8a0ae1ac1356>\", line 23, in <cell line: 1>\n",
            "    File \"<ipython-input-44-ff7074aeca47>\", line 4, in load_model\n",
            "Node: 'save_3/RestoreV2'\n",
            "Key Variable not found in checkpoint\n",
            "\t [[{{node save_3/RestoreV2}}]]\n",
            "\n",
            "Original stack trace for 'save_3/RestoreV2':\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "  File \"<ipython-input-45-8a0ae1ac1356>\", line 23, in <cell line: 1>\n",
            "  File \"<ipython-input-44-ff7074aeca47>\", line 4, in load_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 934, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 946, in build\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 974, in _build\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 543, in _build_internal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 360, in _AddRestoreOps\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 611, in bulk_restore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1522, in restore_v2\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\", line 796, in _apply_op_helper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-ff7074aeca47>:12: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if uninitialized_vars:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[ 0.00085893 -0.00212668  0.00698408 -0.0041001 ]\n",
            " [ 0.00052901 -0.00132071  0.0043053  -0.00253585]\n",
            " [ 0.00079664 -0.00196361  0.00650147 -0.00380023]\n",
            " ...\n",
            " [ 0.0017212  -0.0042304   0.01426833 -0.00826148]\n",
            " [ 0.00030932 -0.00080613  0.00255376 -0.00151637]\n",
            " [ 0.0007027  -0.00171776  0.0058007  -0.00335982]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "dmzJOWWK-jdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_stances = np.argmax(test_pred, axis=1)"
      ],
      "metadata": {
        "id": "m20Qq0Rm-kin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test stances file to get Body IDs\n",
        "test_stances_df = pd.read_csv('competition_test_stances_subset.csv')\n",
        "\n",
        "# Map the stances to integers\n",
        "stance_to_int = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "test_stances_df['Stance'] = test_stances_df['Stance'].map(stance_to_int)\n",
        "\n",
        "# Extract the true stances and Body IDs\n",
        "true_test_stances = test_stances_df['Stance'].values\n",
        "body_ids = test_stances_df['Body ID'].values\n",
        "headline = test_stances_df['Headline'].values\n",
        "\n",
        "# Ensure you have the correct number of predictions\n",
        "assert len(predicted_stances) == len(test_stances_df), \"Mismatch in number of predictions and test stances\"\n",
        "\n",
        "result_df = pd.DataFrame({\n",
        "    'True Stance': true_test_stances,\n",
        "    'Predicted Stance': predicted_stances\n",
        "})\n",
        "\n",
        "# Drop rows with NaN values\n",
        "result_df.dropna(inplace=True)\n",
        "\n",
        "# Extract the true and predicted stances after removing NaNs\n",
        "true_test_stances_clean = result_df['True Stance'].values\n",
        "predicted_stances_clean = result_df['Predicted Stance'].values\n",
        "\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(true_test_stances_clean, predicted_stances_clean)\n",
        "print(\"Accuracy on the test set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDfMP1sf-6l9",
        "outputId": "76ab25cd-b65f-4fbd-d143-2a0ab83b0941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.17625899280575538\n"
          ]
        }
      ]
    }
  ]
}