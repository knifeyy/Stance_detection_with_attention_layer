{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1xgcGK_Vf5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = random.Random()\n",
        "lim_unigram = 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90"
      ],
      "metadata": {
        "id": "tBwf-uKe_2xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_train_instances, file_train_bodies, file_test_instances, file_test_bodies):\n",
        "    train_instances = pd.read_csv(file_train_instances)\n",
        "    train_bodies = pd.read_csv(file_train_bodies)\n",
        "    test_instances = pd.read_csv(file_test_instances)\n",
        "    test_bodies = pd.read_csv(file_test_bodies)\n",
        "    return train_instances, train_bodies, test_instances, test_bodies"
      ],
      "metadata": {
        "id": "S-F7fhcS_6xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_text(data, bodies):\n",
        "    # Convert 'Body ID' columns to the same type (string)\n",
        "    data['Body ID'] = data['Body ID'].astype(str)\n",
        "    bodies['Body ID'] = bodies['Body ID'].astype(str)\n",
        "\n",
        "    # Merge headline and body ID with the actual body text\n",
        "    data = data.merge(bodies, how='left', on='Body ID')\n",
        "    data['combined'] = data['Headline'].astype(str) + ' ' + data['articleBody'].astype(str)\n",
        "    return data"
      ],
      "metadata": {
        "id": "in5i1CtxvAPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_train(train_instances, train_bodies, lim_unigram=lim_unigram):\n",
        "    # Concatenate text for vectorization\n",
        "    train_instances = concatenate_text(train_instances, train_bodies)\n",
        "\n",
        "    # Vectorization\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram)\n",
        "    tfreq_vectorizer = CountVectorizer(max_features=lim_unigram)\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram)\n",
        "\n",
        "    # Fit and transform\n",
        "    bow = bow_vectorizer.fit_transform(train_instances['combined'])\n",
        "    tfreq = tfreq_vectorizer.fit_transform(train_instances['combined'])\n",
        "    tfidf = tfidf_vectorizer.fit_transform(train_instances['combined'])\n",
        "\n",
        "    # Determine the feature size\n",
        "    feature_size = bow.shape[1] + tfreq.shape[1] + tfidf.shape[1]\n",
        "\n",
        "    # Prepare feature set\n",
        "    train_set = np.concatenate([bow.toarray(), tfreq.toarray(), tfidf.toarray()], axis=1)\n",
        "    train_stances = train_instances['Stance'].values\n",
        "\n",
        "    print(\"Train set shape:\", train_set.shape)\n",
        "    print(\"Feature size:\", feature_size)\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, feature_size"
      ],
      "metadata": {
        "id": "IFcU7fKq__k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_test(test_instances, test_bodies, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "    test_instances = concatenate_text(test_instances, test_bodies)\n",
        "    bow = bow_vectorizer.transform(test_instances['combined'])\n",
        "    tfreq = tfreq_vectorizer.transform(test_instances['combined'])\n",
        "    tfidf = tfidf_vectorizer.transform(test_instances['combined'])\n",
        "\n",
        "    test_set = np.concatenate([bow.toarray(), tfreq.toarray(), tfidf.toarray()], axis=1)\n",
        "\n",
        "    print(\"Test set shape:\", test_set.shape)\n",
        "\n",
        "    return test_set"
      ],
      "metadata": {
        "id": "TPwYq2kFAB29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(feature_size, hidden_size, target_size, learning_rate):\n",
        "    # Define placeholders\n",
        "    features_pl = tf.compat.v1.placeholder(tf.float32, [None, feature_size], 'features')\n",
        "    stances_pl = tf.compat.v1.placeholder(tf.int64, [None], 'stances')\n",
        "    keep_prob_pl = tf.compat.v1.placeholder(tf.float32)\n",
        "\n",
        "    # Define the model architecture\n",
        "    hidden_layer = tf.nn.dropout(tf.nn.relu(tf.compat.v1.layers.dense(features_pl, hidden_size)), rate=1-keep_prob_pl)\n",
        "    logits_flat = tf.nn.dropout(tf.compat.v1.layers.dense(hidden_layer, target_size), rate=1-keep_prob_pl)\n",
        "    loss = tf.reduce_sum(tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_flat, labels=stances_pl))\n",
        "\n",
        "    # Optimizer\n",
        "    train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "    return features_pl, stances_pl, keep_prob_pl, logits_flat, loss, train_op"
      ],
      "metadata": {
        "id": "XTt-dk6YAE_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(sess, checkpoint_dir):\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    if latest_checkpoint:\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        try:\n",
        "            saver.restore(sess, latest_checkpoint)\n",
        "            print(\"Model restored from\", latest_checkpoint)\n",
        "        except tf.errors.NotFoundError as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            # Initialize any variables that were not restored\n",
        "            uninitialized_vars = sess.run(tf.compat.v1.report_uninitialized_variables())\n",
        "            if uninitialized_vars:\n",
        "                sess.run(tf.compat.v1.variables_initializer(uninitialized_vars))\n",
        "                print(\"Initialized missing variables.\")\n",
        "    else:\n",
        "        raise ValueError(f\"No valid checkpoint found in the directory: {checkpoint_dir}\")\n"
      ],
      "metadata": {
        "id": "4TxfKN-dAOKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script execution\n",
        "if __name__ == '__main__':\n",
        "    mode = 'load'  # or any other mode you have\n",
        "    model_dir = '/content/model'\n",
        "    hidden_size = 100\n",
        "    target_size = 4\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    # Load and process data\n",
        "    file_train_instances = \"train_stances.csv\"\n",
        "    file_train_bodies = \"train_bodies.csv\"\n",
        "    file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "    file_test_bodies = \"test_bodies.csv\"\n",
        "\n",
        "    raw_train, raw_train_bodies, raw_test, raw_test_bodies = load_data(file_train_instances, file_train_bodies, file_test_instances, file_test_bodies)\n",
        "    train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, feature_size = pipeline_train(raw_train, raw_train_bodies, lim_unigram=lim_unigram)\n",
        "    test_set = pipeline_test(raw_test, raw_test_bodies, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)\n",
        "\n",
        "    if mode == 'load':\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            features_pl, stances_pl, keep_prob_pl, logits_flat, loss, train_op = build_model(feature_size, hidden_size, target_size, learning_rate)\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())  # Initialize all variables first\n",
        "            load_model(sess, model_dir)\n",
        "\n",
        "            # Predict\n",
        "            test_feed_dict = {features_pl: test_set, keep_prob_pl: 1.0}\n",
        "            test_pred = sess.run(logits_flat, feed_dict=test_feed_dict)\n",
        "            print(\"Predictions:\", test_pred)\n"
      ],
      "metadata": {
        "id": "rV50mzVYAQrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3cc3a1-ca4d-4edd-cd8e-150a8614829b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (11998, 15000)\n",
            "Feature size: 15000\n",
            "Test set shape: (25413, 15000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2b2ef7f4912b>:8: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  hidden_layer = tf.nn.dropout(tf.nn.relu(tf.compat.v1.layers.dense(features_pl, hidden_size)), rate=1-keep_prob_pl)\n",
            "<ipython-input-7-2b2ef7f4912b>:9: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits_flat = tf.nn.dropout(tf.compat.v1.layers.dense(hidden_layer, target_size), rate=1-keep_prob_pl)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading checkpoint: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
            "\n",
            "Graph execution error:\n",
            "\n",
            "Detected at node 'save/RestoreV2' defined at (most recent call last):\n",
            "    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    File \"<ipython-input-9-9cbb4522164f>\", line 23, in <cell line: 2>\n",
            "    File \"<ipython-input-8-ff7074aeca47>\", line 4, in load_model\n",
            "Node: 'save/RestoreV2'\n",
            "Key dense/bias not found in checkpoint\n",
            "\t [[{{node save/RestoreV2}}]]\n",
            "\n",
            "Original stack trace for 'save/RestoreV2':\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "  File \"<ipython-input-9-9cbb4522164f>\", line 23, in <cell line: 2>\n",
            "  File \"<ipython-input-8-ff7074aeca47>\", line 4, in load_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 934, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 946, in build\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 974, in _build\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 543, in _build_internal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 360, in _AddRestoreOps\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py\", line 611, in bulk_restore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1522, in restore_v2\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\", line 796, in _apply_op_helper\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 2652, in _create_op_internal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 1160, in from_node_def\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-ff7074aeca47>:12: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if uninitialized_vars:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [[-1.8189484  -0.12561679  0.8685674  -0.13777548]\n",
            " [-0.5439312   0.28486365  0.6298088   0.15852582]\n",
            " [-1.6489872   0.2266714   0.44347236  0.0439118 ]\n",
            " ...\n",
            " [-3.5816772   1.0491668   3.0996      0.7338665 ]\n",
            " [-2.872682    1.0878494   1.0877881  -0.7959989 ]\n",
            " [-2.5751283   1.6066628   1.9447032   0.515922  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "pSyFddHhXa9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_stances = np.argmax(test_pred, axis=1)"
      ],
      "metadata": {
        "id": "I9JHVlhRbFK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test stances file to get Body IDs\n",
        "test_stances_df = pd.read_csv('competition_test_stances.csv')\n",
        "\n",
        "# Map the stances to integers\n",
        "stance_to_int = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "test_stances_df['Stance'] = test_stances_df['Stance'].map(stance_to_int)\n",
        "\n",
        "# Extract the true stances and Body IDs\n",
        "true_test_stances = test_stances_df['Stance'].values\n",
        "body_ids = test_stances_df['Body ID'].values\n",
        "headline = test_stances_df['Headline'].values\n",
        "\n",
        "# Ensure you have the correct number of predictions\n",
        "assert len(predicted_stances) == len(test_stances_df), \"Mismatch in number of predictions and test stances\"\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Body ID': body_ids,\n",
        "    'Predicted Stance': predicted_stances,\n",
        "    'True Stance': true_test_stances,\n",
        "    'Headline': headline\n",
        "})\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "output_file = '/predictions_with_body_ids.csv'\n",
        "predictions_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtoKJH54jZNq",
        "outputId": "5d280eba-71d9-4b24-94e0-d1fd0de5baee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to /predictions_with_body_ids.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.dropna(inplace=True)\n",
        "\n",
        "# Extract the true and predicted stances after removing NaNs\n",
        "true_test_stances_clean = predictions_df['True Stance'].values\n",
        "predicted_stances_clean = predictions_df['Predicted Stance'].values\n",
        "\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(true_test_stances_clean, predicted_stances_clean)\n",
        "print(\"Accuracy on the test set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fpsr7nHn-eI",
        "outputId": "731662fe-5912-45a3-9100-370954d274fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.20473907358243848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Assuming `predicted_stances` and `true_test_stances` are already defined\n",
        "# Define the stance labels\n",
        "stance_labels = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_test_stances_clean, predicted_stances_clean)\n",
        "\n",
        "# Convert the confusion matrix to a DataFrame for better readability\n",
        "cm_df = pd.DataFrame(cm, index=stance_labels, columns=stance_labels)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(true_test_stances_clean, predicted_stances_clean)\n",
        "\n",
        "# Calculate accuracy per class\n",
        "accuracy_per_class = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "# Add a totals row and accuracy column\n",
        "cm_df['Overall'] = cm_df.sum(axis=1)\n",
        "cm_df['% Accuracy'] = accuracy_per_class * 100\n",
        "cm_df.loc['Overall'] = cm_df.sum()\n",
        "cm_df.at['Overall', '% Accuracy'] = overall_accuracy * 100\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(cm_df)\n",
        "\n",
        "# Export the confusion matrix to a CSV file\n",
        "cm_df.to_csv('/mnt/data/confusion_matrix.csv')\n",
        "\n",
        "# Optionally, display the confusion matrix in a more readable format for the notebook\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Confusion Matrix\", dataframe=cm_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "dHJMVpay5iQJ",
        "outputId": "dbe86ca6-710f-4745-91ae-ebb96cebc90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           agree  disagree  discuss  unrelated  Overall  % Accuracy\n",
            "agree        1.0     115.0   1673.0      108.0   1897.0    0.052715\n",
            "disagree     0.0      31.0    628.0       25.0    684.0    4.532164\n",
            "discuss      2.0     225.0   3960.0      259.0   4446.0   89.068826\n",
            "unrelated    1.0     791.0  16243.0     1175.0  18210.0    6.452499\n",
            "Overall      4.0    1162.0  22504.0     1567.0  25237.0   20.473907\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '/mnt/data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5355e9a84951>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Export the confusion matrix to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcm_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/data/confusion_matrix.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Optionally, display the confusion matrix in a more readable format for the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3770\u001b[0m         )\n\u001b[1;32m   3771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3772\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3773\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3774\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         )\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/mnt/data'"
          ]
        }
      ]
    }
  ]
}